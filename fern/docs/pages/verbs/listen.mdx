---
title: Listen
subtitle: Streams audio in real-time over a websocket connection to a third-party websocket server. Stream may be one-way only or bidirectional.
---

```json
{
  "verb": "listen",
  "url": "wss://myrecorder.example.com/calls",
  "mixType" : "stereo"
}
```

### Parameters

<ParamField path="actionHook" type="string" required={true}>
  Webhook to invoke when the listen operation ends.  
  The information will include the duration of the audio stream and a 'digits' property if the recording was terminated by a DTMF key.
</ParamField>

<ParamField path="bidirectionalAudio.enabled" type="boolean" required={false} default="true">
  If true, enable bidirectional audio.  
</ParamField>

<ParamField path="bidirectionalAudio.sampleRate" type="number" required={false}>
  The sample rate of PCM audio sent back to jambonz over the websocket.
</ParamField>

<ParamField path="bidirectionalAudio.streaming" type="boolean" required={false} default="false">
  If true, enable streaming of audio from your application back to Jambonz (and the remote caller).  
</ParamField>

<ParamField path="disableBidirectionalAudio" type="boolean" required={false} deprecated={true}>
  If true, disable bidirectional audio (has the same effect as setting `bidirectionalAudio.enabled to false`).
</ParamField>

<ParamField path="finishOnKey" type="string" required={false}>
  The set of digits that can end the listen action if any one of them is detected.
</ParamField>

<ParamField path="maxLength" type="number" required={false}>
  The maximum length of the listened audio stream, in seconds.  The websocket connection will 
  be closed if this duration is reached.
</ParamField>

<ParamField path="metadata" type="object" required={false}>
  Additional user data to add to the JSON payload sent to the remote server when the WebSocket connection is first established.
</ParamField>

<ParamField path="mixType" type="string" required={false} default="mono">
  `"mono"` (send a single channel), `"stereo"` (send dual channels of both calls in a bridge), or `"mixed"` (send audio from both calls in a bridge in a single mixed audio stream).  
</ParamField>

<ParamField path="passDtmf" type="boolean" required={false} default="false">
  If true, any DTMF digits detected from the caller will be passed over the WebSocket as text frames in JSON format.  
</ParamField>

<ParamField path="playBeep" type="boolean" required={false} default="false">
  Whether to play a beep at the start of the listen operation.  
</ParamField>

<ParamField path="sampleRate" type="number" required={false} default="8000">
  Sample rate of the PCM audio that will be sent from jambonz to remote server.
  Allowable values: `8000`, `16000`, `24000`, `48000`, or `64000`.  
</ParamField>

<ParamField path="timeout" type="number" required={false}>
  The number of seconds of silence that terminates the listen operation.
</ParamField>

<ParamField path="transcribe" type="object" required={false}>
  A nested [`transcribe`](/verbs/verbs/transcribe) verb.
</ParamField>

<ParamField path="url" type="string" required={true}>
  The URL of the remote server to connect to; should be a ws or wss URL.
</ParamField>

<ParamField path="wsAuth.password" type="string" required={false}>
  HTTP basic auth password to use on the WebSocket connection, if desired.
</ParamField>

<ParamField path="wsAuth.username" type="string" required={false}>
  HTTP basic auth username to use on the WebSocket connection, if desired.
</ParamField>

### Audio format

Audio is sent over the websocket in linear 16-bit PCM encoding, using the sample rate specified in the `sampleRate` property.  The audio is sent in binary frames over the websocket connection.
The audio sent back from the server is expected to also be linear16 PCM encoded audio, with a sample rate 
specified in the `bidirectionalAudio.sampleRate` property.

If the `bidirectionalAudio.streaming` property is set to `true`, then the audio sent back from the server should be sent as binary frames 
over the websocket connection and will be streamed to the caller.  Otherwise, audio that is sent back 
is expected to be sent as JSON text frames containing base64-encoded audio content that will be buffered 
and then played out to the caller once it is received in full.

### Initial metadata

One text frame is sent immediately after the websocket connection is established. 
This text frame contains a JSON string with all of the call attributes normally sent on an 
HTTP request (e.g. callSid, etc), plus **sampleRate** and **mixType** properties describing 
the audio sample rate and stream(s).  Additional metadata can also be added to this payload 
using the **metadata** property as described in the table below.  Once the intial text frame 
containing the metadata has been sent, the remote side should expect to receive only binary frames, 
containing audio.  

### Passing DTMF

Any DTMF digits entered by the far end party on the call can optionally be passed to the websocket server 
as JSON text frames by setting the `passDtmf` property to `true`. 
Each DTMF entry is reported separately in a payload that contains the specific DTMF key 
that was entered, as well as the duration which is reported in RTP timestamp units. 
The payload that is sent will look like this:

```json
{
  "event": "dtmf",
  "dtmf": "2",
  "duration": "1600"
}
```

### Bidirectional audio

Audio can also be sent back over the websocket to jambonz. 
This audio, if supplied, will be played out to the caller. 
(Note: Bidirectional audio is not supported when the `listen` is nested 
in the context of a `dial` verb).

There are two separate modes for bidirectional audio:
- non-streaming, where you provide a full base64-encoded audio file as JSON text frames
- streaming, where stream audio as L16 pcm raw audio as binary frames

#### non-streaming

The far-end websocket server supplies bidirectional audio by sending a JSON text frame 
over the websocket connection:
```json
{
  "type": "playAudio",
  "data": {
    "audioContent": "base64-encoded content..",
    "audioContentType": "raw",
    "sampleRate": "16000"
  }
}
```
In the example above, raw (headerless) audio is sent. 
The audio must be 16-bit pcm encoded audio, with a configurable sample rate of either 
8000, 16000, 24000, 32000, 48000, or 64000 khz.  Alternatively, a wave file format can be 
supplied by using type "wav" (or "wave"), and in this case no `sampleRate` property is needed. 
In all cases, the audio must be base64 encoded when sent over the socket.

If multiple playAudio commands are sent before the first has finished playing they will be queued and played in order. You may have up to 10 queued playAudio commands at any time.

Once a `playAudio` command has finished playing out the audio, a `playDone` json text 
frame will be sent over the websocket connection:
```json
{
  "type": "playDone"
}
```
A `killAudio` command can also be sent by the websocket server to stop the playout of 
audio that was started via a previous `playAudio` command:
```json
{
  "type": "killAudio"
}
```
And finally, if the websocket connection wishes to end the `listen`, it can send a 
`disconnect` command:
```json
{
  "type": "disconnect"
}
```

#### Streaming

To enable bidirectional audio, you must explicitly enable it in the listen verb with 
the `streaming` property as shown below:
```js
{
  verb: 'listen',
  bidirectionalAudio: {
    enabled: true,
    streaming: true,
    sampleRate: 8000
  }
}
```

Your application should then send binary frames of linear-16 pcm raw data with the specified 
sample rate over the websocket connection. You can specify both the sample rate that you 
want to receive over the websocket as well as the sample rate that you want to send back audio, 
and they do not need to be the same.  In the example below, we choose to receive 8k sampling 
but send back 16K sampling.
```js
{
  verb: 'listen',
  sampleRate: 8000
  bidirectionalAudio: {
    enabled: true,
    streaming: true,
    sampleRate: 16000
  }
}
```

### Commands
You can send the following commands over the websocket as json frames:
- disconnect
- killAudio
- mark
- clearMarks

#### disconnect
```json
{
  "type": "disconnect"
}
```
This causes the websocket to be closed from the jambonz side, and the associated `listen` verb to end.

#### killAudio
```json
{
  "type": "killAudio"
}
```
This causes any audio that is playing out from the bidirectional socket as well as any buffered audio to be flushed.

#### mark
```json
{
  "type": "mark",
  "data": {
    "name": "my-mark-1"
  }
}
```
You can send a `mark` command if you want to be able to synchronize activities on your end with the playout of the audio stream that you have provided.  Because the audio you provide will usually be buffered before it is streamed, if you want to know when a specific piece of audio has started or completed, send a `mark` command with a name property at the point in the stream you want to sync with.  When that point in the audio stream is later reached during playback, you will get a matching json frame back over the websocket:

```json
{
  "type": "mark",
  "data": {
    "name": "my-mark-1",
    "event": "playout"
  }
}
```

Note that `event` will contain either `playout` or `cleared` depending on whether 
the audio stream reached the mark during playout or the mark was never played out due to 
a `killAudio` command.

#### clearMarks
```json
{
  "type": "clearMarks"
}
```

This command clears (removes) and audio marks that are being tracked. 
When you remove the marks in this way, you will not receive `mark` events for the removed marks.
